import openai
import os
from  dotenv import load_dotenv,find_dotenv
load_dotenv(find_dotenv(),override=True)


#os.environ['OPENAI_API_KEY']=os.getenv('OPEN_API_KEY')
openai.key= os.getenv('OPENAI_API_KEY')


%load_ext jupyter_ai_magics


#### Document loader
def load_document(file):
    import os
    #refactor to a factory
    name,extension = os.path.splitext(file)
    print(f'loading {file}')
    if extension=='.pdf':
        from langchain.document_loaders import PyPDFLoader
        loader = PyPDFLoader(file)
    elif extension=='.docx':
        from langchain.document_loaders import Docx2txtLoader
        loader = Docx2txtLoader(file)
    else:
        print('Document format not supported')    
    data=loader.load()
    return data
    


####Wikipedia loader
def load_from_wikipedia(query,lang='en',load_max_docs=2):
    from langchain.document_loaders import WikipediaLoader
    loader = WikipediaLoader(query=query,lang=lang,load_max_docs=load_max_docs)
    data=loader.load()
    return data


####Split document into chunks of specified size
def chunk_data(data,chunk_size=256):
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)
    chunks = text_splitter.split_documents(data)
    return chunks
    


def print_embedding_cost(texts):
    import tiktoken
    encoding = tiktoken.encoding_for_model('text-embedding-ada-002')
    total_tokens = sum([len(encoding.encode(page.page_content))
                       for page in texts])
    print(f'Total tokens: {total_tokens}')
    print(f'Embedding Cost in USD: {total_tokens/1000*0.0004:.6f}')


####Embed data to pinecone.returns index if already embedded
def insert_or_get_embeddings(index_name,chunks):
    import pinecone
    pinecone.init(api_key=os.environ.get('PINECONE_KEY'),environment=os.environ.get('PINECONE_ENV'))
    from langchain.vectorstores import Pinecone
    from langchain.embeddings.openai import OpenAIEmbeddings
    embedding= OpenAIEmbeddings()
    if index_name in pinecone.list_indexes():
        vector_store = Pinecone.from_existing_index(index_name=index_name,embedding=embedding)
        print(f'{index_name} already exists. Loading embeddings')
    else:
        print(f'Creating index: {index_name} and embeddings',end='')
        pinecone.create_index(index_name,dimension=1536,metric='cosine')
        vector_store=Pinecone.from_documents(chunks,embedding=embedding,index_name=index_name)
        print('OK')
    return vector_store



def delete_pinecone_index(index_name='all'):
    import pinecone
    pinecone.init(api_key=os.environ.get('PINECONE_KEY'),environment=os.environ.get('PINECONE_ENV'))
    if(index_name=='all'):
        indexes=pinecone.list_indexes()
        print('Deleting all indexes.')
        for index in indexes:
            pinecone.delete_index(index)
        print('OK')
        return
    print(f'Deleting index:{index_name}')
    pinecone.delete_index(index)
    print('OK')
    
        


data = load_document('files/2021_1656420953953.pdf')


chunked_data = chunk_data(data)


print_embedding_cost(chunked_data)


delete_pinecone_index()


index_name = 'pvranalytics'
vector_store = insert_or_get_embeddings(index_name=index_name,chunks=chunked_data)


def prompt_vector(question,vector_store):
    from langchain.chains import RetrievalQA
    from langchain.chat_models import ChatOpenAI
    gpt = ChatOpenAI(model="gpt-3.5-turbo",temperature=0)
    vector_retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})
    chain = RetrievalQA.from_chain_type(llm=gpt,chain_type="stuff",retriever=vector_retriever)
    answer = chain.run(question)
    return answer;


def prompt_vector_and_persist(question,vector_store,history=[]):
    from langchain.chains import ConversationalRetrievalChain
    from langchain.chat_models import ChatOpenAI
    gpt = ChatOpenAI(model="gpt-3.5-turbo",temperature=1)
    vector_retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})
    chain = ConversationalRetrievalChain.from_llm(llm=gpt,retriever=vector_retriever)
    result = chain({'question':question,'chat_history':history})
    history.append((question,result['answer']))
    return result,history;


import time
i=1
print('Write quite or Exit to quit.')
while True:
    q=input(f'Queation {i}:')
    i=i+1
    if q.lower() in ['quit','exit']:
        print('Exitng')
        time.sleep(2)
        break
    answer = prompt_vector(q,vector_store)
    print(f'Answer: {answer}')
    print(f'\n{"-"*50}\n')
    


chat_history=[]
question = 'What is the document'
result,chat_history=prompt_vector_and_persist(question,vector_store,chat_history)



chat_history

